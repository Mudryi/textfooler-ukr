{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "168faab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mudryi/phd_projects/textfooler_ukr/textfooler_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Processing synonym dict: 100%|██████████| 256676/256676 [01:01<00:00, 4146.79it/s] \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from dataloader import read_corpus\n",
    "from sentence_sim import SBERT\n",
    "import criteria\n",
    "from prepare_synonym_dict import read_and_clean_synonym_dict\n",
    "import pymorphy2\n",
    "import re\n",
    "from synonym_replacement import replace_word, tokenize_ukrainian, lower_grammar_restrictions, stepwise_inflect\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer(lang='uk')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "path_to_ulif = '/home/mudryi/phd_projects/synonym_attack/synonyms_dictionaries/ulif_clean.json'\n",
    "synonym_dict = read_and_clean_synonym_dict(path_to_ulif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2e7d800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normal_form(word, morph):\n",
    "    parsed = morph.parse(word)\n",
    "    if parsed:\n",
    "        return parsed[0].normal_form\n",
    "    else:\n",
    "        return word\n",
    "    \n",
    "def get_all_synonyms(word):\n",
    "    word = word.lower()\n",
    "    normal_form = get_normal_form(word, morph)\n",
    "\n",
    "    if word in synonym_dict:\n",
    "        return synonym_dict[word]\n",
    "    elif normal_form in synonym_dict:\n",
    "        return synonym_dict[normal_form]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def is_word_token(tok):\n",
    "    return tok.strip().isalpha()  # This excludes whitespace and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8d7cde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory (adv_results_reviews_xml_roberta) already exists and is not empty.\n",
      "Data import finished!\n",
      "Building Model...\n",
      "Model built!\n",
      "Start attacking!\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"/home/mudryi/phd_projects/synonym_attack/cross_domain_uk_reviews/test_reviews.csv\" # \"Which dataset to attack.\"\n",
    "nclasses = 5 # \"How many classes for classification.\"\n",
    "target_model = 'xlm-roberta-base' # \"Target models for text classification: fasttext, charcnn, word level lstm \"\n",
    "target_model_path = \"/home/mudryi/phd_projects/xml-roberta-finetune-reviews/trained_models/tmdk/model_tmdk_7_1000\" #\"pre-trained target model path\"\n",
    "\n",
    "word_embeddings_path = None # \"path to the word embeddings for the target model\"\n",
    "counter_fitting_embeddings_path = None # \"path to the counter-fitting embeddings we used to find synonyms\"\n",
    "counter_fitting_cos_sim_path = None # \"pre-compute the cosine similarity scores based on the counter-fitting embeddings\"\n",
    "\n",
    "SBERT_path = 'sentence-transformers/paraphrase-xlm-r-multilingual-v1' # \"Path to the USE encoder cache.\"\n",
    "\n",
    "output_dir = 'adv_results_reviews_xml_roberta' # \"The output directory where the attack results will be written.\"\n",
    "\n",
    "## Model hyperparameters\n",
    "sim_score_window = 25 # \"Text length or token number to compute the semantic similarity score\")\n",
    "import_score_threshold = -1 # \"Required mininum importance score.\")\n",
    "sim_score_threshold = 0.7 # \"Required minimum semantic similarity score.\")\n",
    "synonym_num = 10 # \"Number of synonyms to extract\"\n",
    "batch_size = 32 # \"Batch size to get prediction\"\n",
    "data_size = 9000 # \"Data size to create adversaries\" reviews have 9663 records\n",
    "perturb_ratio = 0 # \"Whether use random perturbation for ablation study\")\n",
    "max_seq_length = 256 # \"max sequence length for BERT target model\")\n",
    "\n",
    "if os.path.exists(output_dir) and os.listdir(output_dir):\n",
    "    print(\"Output directory ({}) already exists and is not empty.\".format(output_dir))\n",
    "else:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# get data to attack\n",
    "texts, labels = read_corpus(dataset_path)\n",
    "data = list(zip(texts, labels))\n",
    "\n",
    "data = data[:data_size] # choose how many samples for adversary\n",
    "print(\"Data import finished!\")\n",
    "\n",
    "# construct the model\n",
    "print(\"Building Model...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(target_model_path, num_labels=nclasses)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(target_model)\n",
    "\n",
    "def predictor(texts):\n",
    "    \"\"\"\n",
    "    texts: a list of strings, e.g. [\"This is a test\", \"Another sample\"]\n",
    "    Returns: a torch.Tensor of shape (batch_size, nclasses) with probabilities\n",
    "    \"\"\"\n",
    "    if len(texts) > 0 and isinstance(texts[0], str):\n",
    "        texts = [texts]\n",
    "\n",
    "    # Now 'token_lists' is guaranteed to be a list of lists of tokens\n",
    "    # Convert each token-list into one full string\n",
    "    texts = [\" \".join(tokens) for tokens in texts]\n",
    "\n",
    "    # Tokenize with truncation/padding, move to GPU if available\n",
    "    inputs = tokenizer(\n",
    "        texts, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        padding=True\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "    \n",
    "    # Output logits of shape [batch_size, nclasses]\n",
    "    logits = output.logits\n",
    "    \n",
    "    # Convert logits -> probabilities\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    return probs\n",
    "\n",
    "# predictor = model.text_pred\n",
    "print(\"Model built!\")\n",
    "\n",
    "# build the semantic similarity module\n",
    "sbert = SBERT(SBERT_path)\n",
    "\n",
    "# start attacking\n",
    "orig_failures = 0.\n",
    "adv_failures = 0.\n",
    "changed_rates = []\n",
    "nums_queries = []\n",
    "orig_texts = []\n",
    "adv_texts = []\n",
    "true_labels = []\n",
    "new_labels = []\n",
    "log_file = open(os.path.join(output_dir, 'results_log'), 'a')\n",
    "\n",
    "stop_words_set = criteria.get_stopwords()\n",
    "print('Start attacking!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "919f2426",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_predictor=sbert\n",
    "import_score_threshold=-1.\n",
    "# sim_score_threshold = 0.7 # \"Required minimum semantic similarity score.\")\n",
    "# sim_score_window = 25 # \"Text length or token number to compute the semantic similarity score\")\n",
    "# synonym_num = 100 # \"Number of synonyms to extract\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "e93a01c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('До звуку претензій нема, матеріал вироблення хороший. Однак гарантійний талон порожній ... Єдині дані, які в ньому є - гарантійний талон на навушники jbl і перелік моделей. Всі поля на талоні порожні. Чи можу я бути впевненою, що такий гарантій талон приймуть як дійсний? ',\n",
       " 3)"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 8126\n",
    "text_ls, true_label = data[idx]\n",
    "true_label = true_label - 1\n",
    "\n",
    "''.join(text_ls), true_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e874eb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "1b50c875",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_probs = predictor([text_ls]).squeeze()\n",
    "orig_label = torch.argmax(orig_probs)\n",
    "orig_prob = orig_probs.max()\n",
    "\n",
    "if true_label != orig_label:\n",
    "    print(\"Bad\")\n",
    "\n",
    "num_queries = 1\n",
    "len_text = len(text_ls)\n",
    "pos_ls = criteria.get_pos(text_ls)\n",
    "\n",
    "perturbable_indices = [i for i, tok in enumerate(text_ls) if is_word_token(tok) and tok not in stop_words_set]\n",
    "\n",
    "leave_1_texts = [text_ls[:i] + ['<oov>'] + text_ls[i+1:] for i in perturbable_indices]\n",
    "if len(leave_1_texts)==0:\n",
    "    print('no words')\n",
    "\n",
    "leave_1_probs = predictor(leave_1_texts)\n",
    "\n",
    "num_queries += len(leave_1_texts)\n",
    "leave_1_probs_argmax = torch.argmax(leave_1_probs, dim=-1)\n",
    "import_scores = (orig_prob - leave_1_probs[:, orig_label] + (leave_1_probs_argmax != orig_label).float() * (leave_1_probs.max(dim=-1)[0] - torch.index_select(orig_probs, 0, leave_1_probs_argmax))).data.cpu().numpy()\n",
    "words_perturb = []\n",
    "\n",
    "for idx, score in sorted(zip(perturbable_indices, import_scores), key=lambda x: x[1], reverse=True):\n",
    "    try:\n",
    "        if score > import_score_threshold:\n",
    "            words_perturb.append((idx, text_ls[idx]))\n",
    "    except:\n",
    "        print(idx, len(text_ls), import_scores.shape, text_ls, len(leave_1_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "8169d738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('порожній',\n",
       " 22,\n",
       " ['легковажний',\n",
       "  'шалапутний',\n",
       "  'пустодзвонний',\n",
       "  'безшерстий',\n",
       "  'відсутній',\n",
       "  'спорожнений',\n",
       "  'безлюдний',\n",
       "  'спорожнілий',\n",
       "  'безперий',\n",
       "  'голісінький',\n",
       "  'порожнистий',\n",
       "  'спустілий',\n",
       "  'голий',\n",
       "  'кручений',\n",
       "  'малолюдний',\n",
       "  'пустопорожній',\n",
       "  'несолідний',\n",
       "  'фривольний',\n",
       "  'неоперений',\n",
       "  'вітряний',\n",
       "  'легкодухий',\n",
       "  'відлюдний',\n",
       "  'пустинний',\n",
       "  'вітруватий',\n",
       "  'лисий',\n",
       "  'легкодушний',\n",
       "  'легкомисний',\n",
       "  'легкодумний',\n",
       "  'марний',\n",
       "  'плюсклий',\n",
       "  'оголений',\n",
       "  'беззмістовний',\n",
       "  'вільний',\n",
       "  'поверховий',\n",
       "  'нелюдний',\n",
       "  'пустельний',\n",
       "  'полисілий',\n",
       "  'несерйозний',\n",
       "  'бездумний',\n",
       "  'порожніти',\n",
       "  'безпредметний',\n",
       "  'нізчимний',\n",
       "  'пустотілий',\n",
       "  'суєтний',\n",
       "  'пісний',\n",
       "  'малозмістовний',\n",
       "  'незайнятий',\n",
       "  'нагий',\n",
       "  'безпутний',\n",
       "  'безволосий'])"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonyms_all = []\n",
    "for (position, word) in words_perturb:\n",
    "    # Use your custom dictionary-based function\n",
    "    synonyms = get_all_synonyms(word)\n",
    "    synonyms = [synonym for synonym in synonyms if len(synonym.split(' '))==1] #TODO explore to replace word to phrase\n",
    "    synonyms = synonyms[:50]\n",
    "\n",
    "    if synonyms:\n",
    "        synonyms_all.append((word, position, synonyms))\n",
    "\n",
    "text_prime = text_ls.copy()\n",
    "text_cache = text_prime.copy()\n",
    "\n",
    "num_changed = 0\n",
    "\n",
    "idx = matching_indices = [i for i, (word, _, _) in enumerate(synonyms_all) if word == 'порожній'][0]\n",
    "synonyms = synonyms_all[idx]\n",
    "synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "d8891409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad match порожній -> спустілий\n",
      "VERB None\n",
      "[Parse(word='спустілий', tag=OpencorporaTag('ADJF,actv,perf masc,nomn'), normal_form='спустілий', score=1.0, methods_stack=((DictionaryAnalyzer(), 'спустілий', 739, 0),)), Parse(word='спустілий', tag=OpencorporaTag('ADJF,actv,perf masc,accs'), normal_form='спустілий', score=1.0, methods_stack=((DictionaryAnalyzer(), 'спустілий', 739, 4),)), Parse(word='спустілий', tag=OpencorporaTag('ADJF,actv,perf masc,voct'), normal_form='спустілий', score=1.0, methods_stack=((DictionaryAnalyzer(), 'спустілий', 739, 8),))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'До звуку претензій нема, матеріал вироблення хороший. Однак гарантійний талон спустілий ... Єдині дані, які в ньому є - гарантійний талон на навушники jbl і перелік моделей. Всі поля на талоні спустілі. Чи можу я бути впевненою, що такий гарантій талон приймуть як дійсний? '"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(replace_word(''.join(text_prime), text_prime[synonyms[1]], \"спустілий\", morph, debug=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "4553ebfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='зірка', tag=OpencorporaTag('ADJF femn,nomn'), normal_form='зіркий', score=1.0, methods_stack=((DictionaryAnalyzer(), 'зірка', 5, 9),)),\n",
       " Parse(word='зірка', tag=OpencorporaTag('ADJF femn,voct'), normal_form='зіркий', score=1.0, methods_stack=((DictionaryAnalyzer(), 'зірка', 5, 15),)),\n",
       " Parse(word='зірка', tag=OpencorporaTag('NOUN,anim femn,nomn'), normal_form='зірка', score=1.0, methods_stack=((DictionaryAnalyzer(), 'зірка', 9, 0),)),\n",
       " Parse(word='зірка', tag=OpencorporaTag('NOUN,femn,inan nomn'), normal_form='зірка', score=1.0, methods_stack=((DictionaryAnalyzer(), 'зірка', 10, 0),)),\n",
       " Parse(word='зірка', tag=OpencorporaTag('NOUN,inan femn,nomn'), normal_form='зірка', score=1.0, methods_stack=((DictionaryAnalyzer(), 'зірка', 45, 0),))]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph.parse(\"зірка\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cde5ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c981f31f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parse(word='файно', tag=OpencorporaTag('ADVB'), normal_form='файно', score=1.0, methods_stack=((DictionaryAnalyzer(), 'файно', 52, 0),))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammemes = morph.parse(text_prime[synonyms[1]])[0].tag.grammemes \n",
    "cleaned_grammemes = lower_grammar_restrictions(grammemes)\n",
    "\n",
    "stepwise_inflect(morph.parse('файно')[0], cleaned_grammemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f7e35e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2per', 'VERB', 'impf', 'impr', 'sing'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_grammemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e1892d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75052df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # new_texts = [text_prime[:idx] + [synonym] + text_prime[min(idx + 1, len_text):] for synonym in synonyms]\n",
    "# new_texts = [replace_word(' '.join(text_prime), text_prime[idx], synonym) for synonym in synonyms]\n",
    "# new_texts = [x for x in new_texts if x is not None]\n",
    "\n",
    "# if len(new_texts) == 0:\n",
    "#     print(f\"no synonyms for word {text_prime[idx], text_prime}\")\n",
    "\n",
    "# new_probs = predictor(new_texts)\n",
    "# num_queries += len(new_texts)\n",
    "\n",
    "# semantic_sims = sim_predictor.semantic_sim([' '.join(text_cache)] * len(new_texts), [''.join(text) for text in new_texts])[0]\n",
    "\n",
    "# if len(new_probs.shape) < 2:\n",
    "#     new_probs = new_probs.unsqueeze(0)\n",
    "\n",
    "# new_probs_mask = (orig_label != torch.argmax(new_probs, dim=-1)).data.cpu().numpy()\n",
    "# new_probs_mask *= (semantic_sims >= sim_score_threshold)\n",
    "\n",
    "# synonyms_pos_ls = [criteria.get_pos(new_text[max(idx - 4, 0):idx + 5])[min(4, idx)]\n",
    "#                    if len(new_text) > 10 else criteria.get_pos(new_text)[idx] for new_text in new_texts]\n",
    "\n",
    "# pos_mask = np.array(criteria.pos_filter(pos_ls[idx], synonyms_pos_ls))\n",
    "# new_probs_mask *= pos_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87662b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if np.sum(new_probs_mask) > 0:\n",
    "#     text_prime[idx] = synonyms[(new_probs_mask * semantic_sims).argmax()]\n",
    "#     num_changed += 1\n",
    "#     break\n",
    "\n",
    "# else:\n",
    "#     new_label_probs = new_probs[:, orig_label] + torch.from_numpy(\n",
    "#             (semantic_sims < sim_score_threshold) + (1 - pos_mask).astype(float)).float().cuda()\n",
    "#     new_label_prob_min, new_label_prob_argmin = torch.min(new_label_probs, dim=-1)\n",
    "#     if new_label_prob_min < orig_prob:\n",
    "#         text_prime[idx] = synonyms[new_label_prob_argmin]\n",
    "#         num_changed += 1\n",
    "#     text_cache = text_prime[:]\n",
    "# return ' '.join(text_prime), num_changed, orig_label, torch.argmax(predictor([text_prime])), num_queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0120ff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pertubation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textfooler_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
